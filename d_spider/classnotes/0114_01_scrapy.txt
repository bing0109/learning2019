scrapy

简介:
    是一个基于Twisted异步处理的框架,是一个纯python的网络爬虫框架
    
    可用于数据挖掘,监测,和自动化测试.



模块
    engine      负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件
    spiders     Spider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 
    scheduler   调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。
                负责不同请求的先后顺序,放在请求队列里面,有去重的功能
                请求队列,可以是 由优先级的散列
        
    downloader  下载器负责获取页面数据并提供给引擎，而后提供给spider。
                请求队列依次通过engine给downloader与网络进行交互发送请求,获取响应信息,响应信息又由engine发送给spider进行提取信息,提取的信息经engine发送给item pipeline进行处理,包括格式化(如字符取固定长度),数据保存等
                同时,spider根据响应确定下一步应该发送的请求,并发给engine,进行新一轮处理
    
    ItemPipeline负责处理被spider提取出来的item,典型的处理有清理,验证,持久化,例如存取数据到数据库中
    
    中间件
        Downloader Middleware    
                下载器中间件是在引擎及下载器之间的特定钩子，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。
                engine和downloader之间的中间件,主要处理,代理/ip伪装/捕获处理异常等,直接通过selenium进行请求和响应,就可以不通过downloader模块了

数据流
    1. engine打开一个网站,找到处理该网站spider并向该spider请求第一个要爬取的url(s)
    2. engine获取到第一个要爬取的url后,传递给scheduler以Request调度
    3. engine向scheduler请求下一个要爬取的url
    4. scheduler返回下一个要爬取的url给engine, engine把url通过Downloader Middleware转发给downloader
    5. downloader完成请求并得到响应后,生成一个该页面的Response,并通过Downloader Middleware发生给engine
    6. engine接收到Response后通过 Spider Middleware发送给spider处理
    7. spider处理Response并返回爬取到的Item给engine,并且跟进新的Request给engine
    8. engine把spider爬取到item传递给ItemPipeline, 把spider返回的新的Request传递给scheduler
    9. 重复第2-8步,直至scheduler没有更多的request,engine就关闭该网站连接.



安装:
    scrapy依赖的库比较多，至少需要依赖的库有Twisted 14.0、lxml 3.4和pyOpenSSL 0.14

    pip install scrapy
    



启动:
    创建一个srapy项目
        scrapy startproject myscrapy
    
    创建一个具体的spider
        cd myscrapy
        scrapy genspider quotes quotes.toscrape.com
            quotes  定义的spider名称
            quotes.toscrape.com     定义的spider起始请求域名
            在myscrapy目录下会多了一个quotes.py文件
            
    运行:
        scrapy crawl quotes
     
        scrapy crawl quote -o quotes.json
            quote   是要爬取的项目名称
            -o quotes.json表示输出到quotes.json文件,output,
                可输出格式'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
     







主要目录介绍

    ├── myscrapy            # 项目目录
    │   ├── items.py        # 保存数据的容器，使用方法类似于字典，定义要提取和保存的字段名称
    │   ├── middlewares.py  # 中间件
    │   ├── pipelines.py    # 对提取处理的item进行进一步处理，定义数据保存等，可以在里面定义自己的itempipline
    │   ├── settings.py     # 设置
    │   └── spiders
    │       └── quote.py    # spider代码文件，定义spder起始url等，里面的parse()方法可以直接对Response里面的内容进行解析，包括翻页处理的逻辑等
    └── scrapy.cfg



编写自己的itemPipeline
    参考:https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html
    
    process_item(self, item, spider)
        每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem 异常，被丢弃的item将不会被之后的pipeline组件所处理。
        参数:	
            item (Item 对象) – 被爬取的item
            spider (Spider 对象) – 爬取该item的spider


    open_spider(self, spider)
        当spider被开启时，这个方法被调用。

        参数:	
            spider (Spider 对象) – 被开启的spider
            
            
    close_spider(spider)
        当spider被关闭时，这个方法被调用

        参数:
        	spider (Spider 对象) – 被关闭的spider
        	
        	
    from_crawler(cls, crawler)
        If present, this classmethod is called to create a pipeline instance from a Crawler. It must return a new instance of the pipeline. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for pipeline to access them and hook its functionality into Scrapy.
        这个的作用现在还不是很明白
        参数:
        	crawler (Crawler object) – crawler that uses this pipeline



settings里面定义的变量名要用大写
























































        
        
