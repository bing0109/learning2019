逻辑回归
    主要解决分类问题
    
    线性逻辑回归
    
    非线性逻辑回归
    
    
    多分类问题
        方法1 One vs. Rest 回归
            每次抽取一个类别作为1，其他所有类别作为0，依次分类，就可以逐步找出所有类别
            针对一个测试样板，找到函数输出值最大（最接近1）的那一个，就可以确定样本类别
            
        方法2 softmax分类器
        
        
            相对来说，方法2比方法1的训练时间长，但效果好一下
            


    类不平衡
        训练样本中可能出现y=1占比90%，y=0占比10%，会导致学习模型偷懒，实际学习过程中损失函数主要受占比大的样本影响
        接近办法
            数据量大--下采样，按占比小的采样若干次，直至占比大的数据都被采集一次
            数据量小--上采样，按占比大的采样一次，占比小的采样多次，直至两类数据数量一致
                        SMOTE造数据
                            根据现有数据，按附近原则，构造分类占比小的数据，直至与占比大的数据数量一样
                            
            修改损失函数
                把损失函数各部的权重 按样本各类数量占比来修改



    数据预处理
        归一化



  
