1.爬虫的基础
1.1 http过程
在浏览器输入目标的url(名称、地址)，会生成一个请求，根据url提供的地址（ip地址），将请求发送给目标服务器，服务器根据请求的内容，会生一个响应，将这个响应回传给浏览器，浏览器将获得的响应进行解析。
1.2 http请求过程
1.2.1 请求方式
get请求：适合传输或者获取一些私密性不强的内容；（对应的url里面的参数适合公开的）
post请求：提交表单，例如：传输账号和密码
1.2.2 请求url
url:统一资源定位符，他有两个作用：第一个作用起到‘名称’的作用，另外一个主要是提供‘地址’的作用（ip地址）
1.2.3 请求头
user-agent:主要包含用户使用浏览器版本情况，目标服务器可以基于此来判定，当前的请求是机器访问，还是正常访问；fake-useragent
referer:说明的是当前网页的是基于哪一个网页来的
cookie:记录了一些会话信息，当然也包含了账号登录的信息，所以经常用来做模拟登陆
1.2.4 请求体
post请求有请求体，get请求没有请求体

1.3 http响应
1.3.1 状态码
200：请求成功
3**：重定向（301/302）
4**：客户端存在问题，404（Not Found）
5**：服务器的异常
1.3.2 响应头
也可以包含cookie信息，提取cookie信息可以用来做cookie池
1.3.3 响应体
爬虫里面主要的感兴趣的信息

1.4 爬虫的概念
爬虫：是一个获取网页、提取信息和保存信息一个自动化过程
1.4.1 获取网页
模拟浏览器的请求过程，获得响应。主要的工具：urllib、requests
1.4.2 提取信息
针对获得的响应体，去提取感兴趣的信息
html:正则表达式、CSS选择器、Xpath选择器,主要工具：re、BeautifulSoup、lxml、pyquery
json格式：主要工具json转化为字典
二进制：图片、音乐、视频
1.4.3 保存
文件：txt/csv/json/xml
数据库：MySql、MongoDB、Redis

2. Urllib
urllib.request:请求模块，主要是用来模拟请求
urllib.parse:解析模块，主要针对url进行解析和编码
urllib.error：异常处理
2.1 urllib.request
2.1.1 请求
response=urllib.request.urlopen(url,data=None,timeout)#url:请求的url,data:请求体，必须接受的bytes类型的数据，timeout超时的设置
2.1.2 Request对象
Request对象主要是用来加入请求体，然后输入到urlopen里面完成请求
urllib.request.Request(url,data=None,headers={},timeout，method='Get')#headers:请求体，可以接受字典数据,method:指明请求方式

2.2 响应
response.status#状态码
response.getheaders()#获得响应头
response.getheader('')#获得指定响应头的值
response.read()#获得响应体，返回的是二进制，经常可以和decode('utf-8')一起使用，转化为字符串
response.read().decode('utf-8)

2.3 urllib.parse
urllib.parse.quote(str)#主要针对字符串进行url编码
urllib.parse.unquote(str)#url反编码
urllib.parse.urlencode(dict)#主要是针对字典进行url编码

2.4 urllib.error
urllib.error.URLError:关于URL方面的异常，最基层的异常，只能返回reason
urllib.error.HTTPError:是URLError的自类，可以返回code、reason、headers

3. requests
3.1 请求方式
requests.get()#get请求
requests.post()#post请求
3.2 requests.get
response=requests.get(url,params={},headers={},proxies={},timeout)#headers:请求头，proxies：代理，timeout:超时设置
#url:请求的url,可以是部分（params不为空），也可以是全部（params=None）
3.3 requests.post
response=requests.get(url,params={},headers={},proxies={},timeout,data={})#data:请求体，接受字典的格式
3.4 响应
response.status_code#状态码
response.headers#响应头
response.cookies

#响应体
response.text#获得字符串
response.content#获得是二进制
response.json()#获得的是字典

3.5 异常
from requests import exception
exception.RequestException:最底层的异常
exception.HttpError:继承与RequestException，关于http方面的异常
exception.ConnectionError:继承与RequestException，关于Connection方面的异常（404）
exception.TimeOut:继承与RequestException，关于超时方面的异常

4.正则表达式
4.1 常用的规则
\d:数字，等价于[0-9]
\s:空格字符，\t\n\f\r
\w:字母、数字和下划线
.:除换行符以外的任意字符
*：0个或者多个
+：1个或者多个
[...]:中括号里面的任意一个
.*?:通用匹配，匹配字任意多个字符，一般用来匹配字符串中间的字符，非贪婪模式
.*:通用匹配，贪婪模式
\:转义字符
#修饰符
re.S:可以让‘.’匹配包括换行符在内的任意字符
re.I:匹配时对大小写不敏感
4.2 re.match 
#从头开始匹配
re.match(r'',str,re.S|re.I)
4.3 re.search()#从任意位置开始匹配，返回满足要求的第一个
4.4 re.findall()#返回所有满足要求的匹配
4.4 re.sub(r'',target,str)#将满足匹配要求的字符替换成‘target’
4.4 re.compile(r'',re.S)#编译匹配规则，便于复用

5.BeautifulSoup
from bs4 import BeautifulSoup
5.1 解析器
soup = BeautifulSoup(html,'html.parser')#将html字符串，解析成带结构特点的BeautifulSoup对象，常用的解析器有：html.parser/lxml
5.2 节点选择器
a=soup.p#定位标签名为'p'第一个节点
soup.div.p
5.3 获取信息
a.string/s.get_text()#获取文本信息
a.attrs['href']/a['href']#获取‘href’属性信息，a.attrs输出的是属性字典
5.4 关联选择器
soup.p.contents/soup.p.children#子节点
soup.p.descendants#获取子孙节点
soup.p.parent#获取父节点
soup.p.parents#获取祖先节点
soup.p.next_sibling#获取下一个兄弟节点
soup.p.previous_sibling#获取上一个兄弟节点
soup.p.next_siblings#获取下面所有兄弟节点
soup.p.previous_siblings#获取上面所有兄弟节点
5.5 方法选择器
soup.find_all(name,attrs={},text)#返回的所有满足要求的节点，返回的是列表
#name：标签名字；attrs:属性；text:节点内部的文本，也可以用re.compile匹配
soup.find(name,attrs={},text)#返回的第一个满足要求的节点，其他的忽略
5.6 CSS选择器
soup.select('.class #id p')

6.selenium
6.1 声明浏览器
form selenium import webdriver
browser = webdriver.Chrome()#声明Chrome浏览器
6.2 访问网页
browser.get('http://www.baidu.com')
6.3 节点选择
from selenium.webdriver.common.by import By
#定位满足要求的第一个节点
browser.find_element(By.CSS_SELECTOR,'.class #id p')
browser.find_element_by_css_selector('.class #id p')
#定位所有满足要求的节点
browser.find_elements(By.CSS_SELECTOR,'.class #id p')
browser.find_elements_by_css_selector('.class #id p')
6.4 节点交互
input.send_keys(keyword)#对输入框输入关键字
input.clear()#清空文本框
button.click()#点击按钮
6.5 动作链
from selenium.webdriver.common.action_chains import ActionChains
action = ActionChains(browser)#声明动作链
....需要执行的动作，例如拖拽，双击
action.perform()#执行动作
6.6 延时等待
延时等待是为等待查找的节点是否已经加载好，如果没有加载好，则会等待相应的时间
6.6.1 隐式等待
browser.implicitly_wait(10)
6.6.2 显式等待
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
wait = WebDriverWait(browser,10)#声明显式等待
wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'.class #id p')))#等待通过CSS选择定位出来的节点，是否已经加载好，如果没有加载好，就等待响应时间
#常用的一些等待条件：
presence_of_element_located：判断节点是否已经加载
element_to_be_clickable：判断节点可点击
text_to_be_present_in_element：判断节点里面是否存在某段字符
6.7 cookies
browser.get_cookies()#获取当前浏览器的cookies
browser.delete_all_cookies()#清空cookies
browser.add_cookie()#添加cookies
6.8 前进和后退
browser.back()#后退上一次访问页面
browser.forward()#前进
6.9 选项卡
browser.execute_script('window.open()')#打开选项卡
browser.switch_to_window(browser.window_handles[1])#切换到1窗口

6.10 关于异常
from selenium.common.exceptions import NoSuchElementException,TimeoutException
NoSuchElementException:用find_element/find_elements没有找到对应的节点，则会抛出该异常
TimeoutException：在显式等待时，超过了等待时间
