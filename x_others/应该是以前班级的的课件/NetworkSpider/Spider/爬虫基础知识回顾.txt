1. 爬虫的基本原理
1.1 http的原理
从浏览器的里面输入我们感兴趣的url,浏览器跟url生成一个请求，指向目标Server,Server会根据请求，返回一个响应给浏览，浏览器对响应进行解析。
1.2 请求
1.2.1 请求方式
get请求方式
post请求方式
1.2.2 请求url
url:里面会有一些参数，一般放在‘？’的后面，多个参数，通过‘&’进行连接
1.2.3 请求头
user-agent：最基本的反爬，服务器可以以此来进行判断是正常访问还是机器访问
content-type:不同的提交数据的方式
refer:当前网页的来源
cookies:主要记录当前会话的信息，（账号、密码等）
1.2.4 请求体
get方式没有请求体，post有请求体
1.3 响应
1.3.1 状态码
200：请求成功，301/302:重定向，404：不存在，5**:server内部错误
1.3.2 响应头
包含cookies对应的信息
1.3.3 响应体
最感兴趣的部分，需要的爬取的信息包含在该部分。html源码：正则方式、BeautifulSoup、lxml、pyquery;json的字符串：json.loads()转化为字典；字节流：进行保存为对应的格式
1.4爬虫
爬虫：就是模拟浏览器进行请求获得响应，并对响应体信息进行提取，将获得的信息进行保存的过程
(1)获得响应：有对应的请求库urllib/requests,渲染库selenium
(2)提取信息：针对响应体进行信息提取。html源码：正则方式、BeautifulSoup、lxml、pyquery;json的字符串：json.loads()转化为字典；字节流：进行保存为对应的格式
(3)保存信息：保存为txt、csv等文件格式，或者直接保存到数据库里面

2. urllib
(1)urllib.request:请求部分，主要是用来完成请求，获得响应
(2)urllib.parse:解析url
(3)urllib.error:处理异常
2.1 urllib.request
request1=urllib.request.Request(url,data=None,headers={},method='GET',timeout=None)#构建请求Request对象
#url:请求的url
#data:请求体，只能接受字节流（bytes）的数据,可以通过urllib.parse.urlencode辅助进行转换
#headers:请求头
#method:请求方式
response=urllib.request.urlopen(request1)
2.2 获得响应
response.status#获得状态码
response.get_headers()#获得响应头
response.get_header('key')#获得响应头
response.read()#获得响应体，返回时二进制，可以通过decode()进行解码
2.3 urllib.error
urllib.error.URLError:urllib.error里面总的异常，只返回reason
urllib.error.HTTPError:是URLError的紫烈，返回reason/code/header
2.4 urllib.parse
urllib.parse.urlencode()#针对字典，进行url编码，将不同的key-value通过‘&’进行拼接
urllib.parse.quote()#针对字符串进行url编码
urllib.parse.unquote()#quote()的逆方法

3.requests
3.1 请求方式
requests.get()#get请求
requests.post()#post请求
3.2 requests.get()
response=requests.get(url,params={},headers={},cookies=RequestsCookieJar,proxies={})
#url:请求url
#params:url的参数，字典形式
#headers:请求头，字典形式
#cookies：只能接受requests.cookie.RequestsCookieJar的形式
#proxies：代理，字典形式
3.3 requests.post()
response=requests.post(url,params={},headers={},data={},cookies=RequestsCookieJar,proxies={},files={})
#data:请求体，字典形式
#files：上传文件，字典形式
3.4 获得响应
response.status_code#获得状态码
response.headers#获得响应体
response.cookies#获得cookies
#响应体
response.text#获得字符串
response.content#获得二进制
response.json()#如果响应体是类似json的字符串，输出的是字典
3.5 异常
requests.exception.RequestException#最基础的异常
requests.exception.HTTPException#http方面的额异常
requests.exception.ConnectionException#连接方面的异常
requests.exception.TimeOut#超时的异常

4.正则表达式
4.1 常用的匹配规则
\d#匹配数字，等价于[0-9]
\s#空格，\t\n\r
.#除换行符以外的任意字符
*#*前面的规则0个或多个
+#一个或者多个
.*?#非贪婪模式，一般放在中间
.*#贪婪模式，一般用来匹配结尾的内容
[]#满足'[]'内任意一个
()#分组
？#0或1
4.2 re.match
re.match(r'',html,re.S|re.I)#从头开始匹配
#修饰符
re.S：表示‘.’可以用来表示换行符
re.I:对大小写不敏感
4.3 re.search(r'',html,re.S|re.I)#从任意位置开始匹配，只返回满足匹配的第一个，其他的忽略
4.4 re.findall(r'',html,re.S|re.I)#获取所有满足匹配的结果，返回的是一个列表
4.5 re.sub(r'','',html,re.S)#替换满足匹配的内容
4.6 re.compile(r'',re.S)#对匹配规则进行打包，便于复用

5.BeautifulSoup
5.1 基本用法
from bs4 import BeautifulSoup
soup = BeautifulSoup(html,'lxml')#主要是用来将html字符串，转换成带有结构特点的BeautifulSoup对象，解析器可以用‘lxml’，用python自带的‘html.parser’
5.2 获取信息
#a为定位到的某个节点
a.string/a.get_text()#获取文本信息
a.attrs['name']/a['name']#获取‘name’属性的信息
5.3 节点选择器和关联选择器
soup.p#定位到第一个p标签对应的节点
soup.p.children/soup.p.contents#获取子节点
soup.p.descendants#获取子孙节点
soup.p.parent#获取父节点
soup.p.parents#获取祖先节点
soup.next_sibling#获取一个弟弟的节点
soup.next_siblings#获取所有的弟弟节点
soup.previous_sibling#获取哥哥节点
soup.previous_siblings#获取所有哥哥节点
5.4 方法选择器
find_all(name='',attrs={},text)#返回所有满足要求的节点,返回的是列表
#name:节点对应的标签的名字
#attrs:属性
#text:文本信息，或者是正则表达式（re.complie）
find（name='',attrs={},text）#返回满足要求的第一个节点，其他的忽略
5.5 CSS选择器
soup.select('.class #id p')

6. selenium
6.1 声明浏览器
from selenium import webdriver

browser = webdriver.Chrome()
6.2 访问网址
browser.get('http://www.baidu.com')
6.3 查找节点
#查找返回满足要求的第一个节点
a=browser.find_element_by_css_selector('.class #id p')
a=browser.find_element_by_id('id')
from selenium.webdriver.common.by  import By
a=browser.find_element(By.CSS_SELECTOR,'.class #id q')
#返回所有满足要求的节点
browser.find_elements(By.CSS_SELECTOR,'.class #id q')#返回的是列表
6.4 获取信息
6.4.1获取响应信息（）
browser.pagesource#js渲染之后的响应体信息
browser.current_url#目前浏览器访问到的网页的url
6.4.2 获取文本信息
a.text
6.4.3 获取属性信息
a.get_attribute('href')#获取定位到的a节点里面的href信息

6.5 cookies
browser.get_cookies()#获得cookie
browser.delete_all_cookies()#清空cookies
browser.add_cookie()#添加cookies

6.6 节点交互
inputs.send_keys('keyword')#输入信息：inputs为定位到的文本框，keyword为输入信息
inputs.clear()#清空文本框

button.click()#点击按钮：button为定位到的按钮

6.7 动作链
from selenium.webdriver import ActionChains
action = ActionChains(browser)
...可以是常规的一些执行动作，例如拖拽，回车等
action.perform()

6.8 延时等待：对应查找的节点，还有渲染出来，所以通过延时等待一段时间，等该节点出现之后，才继续执行之后的程序
6.8.1 隐式等待
browser.implicitly_time(10)
6.8.2 显式等待
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

wait = WebDriverWait(browser,10)
wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'#id .class q')))
常用的一些等待条件：
presence_of_element_located#判断某个节点是都已经存在
element_to_be_clickable#判断某个节点是否已经能够被点击
text_to_be_present_in_element#某个节点文本包含某文字

6.9 异常
from webdriver.common.exception import NoSuchElementException,TimeOutException
NoSuchElementException#查找节点，如果没有找到该节点出现的异常
TimeOutException#显示延时等待，如果在规定的时间内，没有满足条件，则会出现该异常。

6.10 其他
browser.forward()#前进
browser.back()#后退

browser.execute_script('window.open()')#打开选项卡
browser.switch_to_window(browser.window_handles[1])#切换到1窗口